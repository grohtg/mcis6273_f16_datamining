{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output: \n",
    "  pdf_document:\n",
    "    citation_package: natbib\n",
    "    keep_tex: true\n",
    "    fig_caption: true\n",
    "    latex_engine: pdflatex\n",
    "    template: svm-latex-ms.tex\n",
    "title: \"Assignment 2: Clustering and Basic Classification\"\n",
    "thanks: \n",
    "author:\n",
    "- name: Keith Maull\n",
    "  affiliation: Adjunct Instructor / Southern Arkansas University\n",
    "abstract: \"In this homework you will apply some of what you have learned about clustering and classification through the extended use of Scikit-learn, Numpy and Pandas.\"\n",
    "keywords: \"clustering, k-means, classification, data analysis, agglomerative clustering\"\n",
    "date: \"`r format(Sys.time(), '%B %d, %Y')`\"\n",
    "geometry: margin=1in\n",
    "fontfamily: mathpazo\n",
    "fontsize: 11pt\n",
    "# spacing: double\n",
    "bibliography: \n",
    "biblio-style: apsr\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 / Clustering and Basic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__This assignment is worth up to 30 POINTS to your grade total if you turn it in on time.  Late assignments will lose 15%.__\n",
    "\n",
    "\n",
    "| Points Possible | Due Date | Expected Time Commitment |\n",
    "|:---------------:|:--------:|:------------------------:|\n",
    "| 30 | Friday November 11 @ Midnight| 4-8 hours |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVES\n",
    "* Learn some of the clustering features of [Scikit-Learn](http://scikit-learn.org/stable/modules/clustering.html) to do partitioning and hierarchical clustering ($k$-Means and hierarchical clustering algorithms);\n",
    "* Learn about document clustering, and document similarity scoring using TFIDF;\n",
    "* Using built-in $k$-nearest neighbor and interpret the output in Scikit-learn -- this is an extension of the implementation you did last time;\n",
    "* Learn how to binarize categorical variables in Scikit-learn;\n",
    "* Learn how to use `DecisionTreeClassifier` to build a basic classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHAT TO TURN IN\n",
    "\n",
    "You are being encouraged to turn the assignment in using the provided Jupyter Notebook.  To do so, clone the repository and modify the `Homework2.ipynb` file in the `HOMEWORK/02` directory.\n",
    "\n",
    "\n",
    "Turn in a copy of a ipynb file OR a PDF or Word Document to Blackboard with the answers to the questions labeled with the &#167; sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESOURCES\n",
    "\n",
    "| What | Where |\n",
    "|:----------------------------------:|:---------------------:|\n",
    "| SciPy clustering documentation | [SciPy.org](http://scikit-learn.org/stable/modules/clustering.html) |\n",
    "| A very recent (Oct. 3, 2016) write up on Data Mining in Python. | [Data Mining and Algorithms (Springboard Education)](https://www.springboard.com/blog/data-mining-python-tutorial/) |\n",
    "| A nice writeup on $k$-means. | [Clustering With K-Means in Python (The Data Science Lab)](https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/) |\n",
    "|TFIDF in Scikit-Learn. |[Scikit-Learn Text Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)|\n",
    "|**DATASET** of top 1000 movies on IMDB by user ratings.  This data has been stored in the [data directory](./data). | [IMDB Top 1000 @ icheckmovies.com](https://www.icheckmovies.com/lists/imdb+top+1000/lampadatriste/) |\n",
    "|**API** that is used to access IMDB movie metadata. | [OMDB API at omdbapi.com](http://omdbapi.com/) |\n",
    "|**DATASET** of consumer debt complaints. | [US Federal Consumer Financial Protection Bureau](http://www.consumerfinance.gov/data-research/consumer-complaints/) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO COMPLETE THIS ASSIGNMENT + ASSIGNMENT DETAILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA FOR PART 1\n",
    "\n",
    "We will be using data from [IMDB](http://imdb.com) and working with movie data.  IMDB is a movie database that is widely used to learn about (and rate) movies.  Much of the work around movies focuses on predicting ratings -- for example, the [Netflix Prize contest](http://netflixprize.com/) was designed to encourage developers to explore better algorithms for rating movies.  Instead of predicting ratings, we will work instead with **clustering the plots of movies**.  Data will come from the [OMDB API](http://omdbapi.com/) which allows a developer to extract information from IMDB programmatically since there is no open public API directly published by IMDB. \n",
    "\n",
    "You can view the [notebook here](./preliminaries.ipynb) to see how the data was extracted, but you can skip that step and look directly at the file which is the output of that data.  Also, you can find the data for this assignment in [`data`](./data) directory, and in it you will see a TSV file called \n",
    "\n",
    "* [data/top1000_movie_summaries.tsv](./data/top1000_movie_summaries.tsv).\n",
    "\n",
    "In that file, you will notice 1000 movie summaries -- the first column is the movie name, the second is the plot summary from OMDB and looks like this:\n",
    "\n",
    "```txt\n",
    "...\n",
    "The Shawshank Redemption\tChronicles the experiences of a formerly successful banker as a prisoner in the gloomy jailhouse of Shawshank after being found guilty of a crime he did not commit. The film portrays the man's unique way of dealing with his new, torturous life; along the way he befriends a number of fellow prisoners, most notably a wise long-term inmate named Red.\n",
    "The Godfather\tWhen the aging head of a famous crime family decides to transfer his position to one of his subalterns, a series of unfortunate events start happening to the family, and a war begins between all the well-known families leading to insolence, deportation, murder and revenge, and ends with the favorable successor being finally chosen.\n",
    "The Godfather: Part II\tThe continuing saga of the Corleone crime family tells the story of a young Vito Corleone growing up in Sicily and in 1910s New York; and follows Michael Corleone in the 1950s as he attempts to expand the family business into Las Vegas, Hollywood and Cuba.\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKGROUND FOR PART 1 \n",
    "Document clutstering is a common task in text mining and has broad applications in a variety of contexts.  In the unsupervised context, such clustering provides insights into a set of documents and the common features they share.  In the supervised context such clustering allows one to train and subsequently classify documents.  For example, if one were to determine of a document is of a certain kind (e.g. legal, academic) one can use labeled instances to learn the features that would allow the discrimination of unlabeled/unseen instances.\n",
    "\n",
    "There are several good resources in information retrieval that you may want to bookmark for future reference in text mining and information retrieval generally:\n",
    "\n",
    "* Manning, C.D., Raghavan, P. and Sch√ºtze, H. (2008) Introduction to Information Retrieval.  doi:  http://dx.doi.org/10.1017/CBO9780511809071;  Available at: [Stanford NLP - Information Retrieval](http://nlp.stanford.edu/IR-book/information-retrieval-book.html).\n",
    "\n",
    "\n",
    "\n",
    "#### DOCUMENT ANALYSIS: TERM FREQUENCY (TF) AND INVERSE DOCUMENT FREQUENCY (TF)\n",
    "\n",
    "The intuition behind analyzing words in documents hinges on the following:\n",
    "\n",
    "* terms that are frequent _in documents_ are given higher importance than those that are infrequent\n",
    "* terms that are frequent _across_ documents are not considered as important\n",
    "\n",
    "that is _common_ words across an entire corpus are *discounted* while \n",
    "those that are _common_ within documents are *boosted*.\n",
    "\n",
    "\n",
    "#### DOCUMENT ANALYSIS: TERM FREQUENCY INVERSE DOCUMENT FREQUENCY (TFIDF)\n",
    "\n",
    "To realize the above, we will introduce a concept all TFIDF, which is composed of two parts, the TF (or **term frequency**) and the IDF (**inverse document frequency**).\n",
    "\n",
    "Let's break these two down and then put them back together.\n",
    "\n",
    "**Term frequency (TF)** is a simple concept and is exactly as it says: the _counts_ of terms in a document.  So for a term (word) $t$ and document $d$, the TF is just the number of occurences of $t$ in $d$,\n",
    "\n",
    "$$\\textrm{tf}(t,d) = \\big| t \\in d \\big|$$\n",
    "\n",
    "**Inverse document frequency (IDF)** provides a way to determine if a terms is rare or common given _all_ documents $D$, and is logarithmically scaled so rare terms avoid completely disappearing.  Thus,\n",
    "\n",
    "$$\n",
    "\\textrm{idf}(t,D) = { \\big| D \\big| \\over {1 + \\big| \\{t \\in d | d \\in D \\} \\big| } }\n",
    "$$\n",
    "\n",
    "**TFIDF** is thus: for a set of documents (corpus) $D$ and document $d \\in D$ and terms $t \\in d$,\n",
    "\n",
    "$$\n",
    "\\textrm{tfidf}(t,d,D)= \\textrm{tf}(t,d,D) \\cdot \\textrm{idf}(t,D)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF in ScikitLearn\n",
    "Luckily we need only recall the details of the notions of these techniques and not implement them.  In Scikitlearn, most of the machinery of TFIDF is done for us using `CountVectorizer`, [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A TFIDF Example\n",
    "\n",
    "Let us consider loading the first 50 plots of the plots file.  The code below will read the first 50 lines of the plot summary file and store them in a `dict` for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "plots_50 = {}\n",
    "with open(\"./data/top1000_movie_summaries.tsv\") as fi:\n",
    "    tsvReader = csv.reader(fi, delimiter='\\t')\n",
    "    for i, (title, plot) in enumerate(tsvReader):\n",
    "        plots_50[title]=plot\n",
    "        if i == 49:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's a Wonderful Life\",\n",
       " 'Seven Samurai',\n",
       " 'The Godfather: Part II',\n",
       " 'The Green Mile',\n",
       " 'Modern Times',\n",
       " 'Se7en',\n",
       " 'The Matrix',\n",
       " 'Sunset Boulevard',\n",
       " 'Once Upon a Time in the West',\n",
       " 'Memento']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plots_50.keys()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we're going to use some built in implementations of Tfidf to do some interesting things with the docoment data.  Recall what we want is to create a way to represent all terms in the document, and then from there score the terms over _all documents_ and _within documents_.\n",
    "\n",
    "**IMPORTANT:** You should read the documentation in Scikit-learn about the [TfidfVectorizer]() to dig in deeper on this issue.\n",
    "\n",
    "**NOTE:** as input to the vectorizer, we must pass all the plot summaries dictionary given by `values()` which returns the dictionary values for all keys _in order_ they are stored in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "# create the tfidf matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(plots_50.values())\n",
    "\n",
    "# convert the matrix to an array - which reduces the sparseness of the matrix\n",
    "tfidf_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we can do is see the terms and their tfidf scores.\n",
    "\n",
    "Let us consider the first find out what the terms of our corpus is -- recall these are just all the terms of the _plot summaries we have seen_, which are represented in the `plots_50` variable.\n",
    "\n",
    "Since there are **many** terms, we will just display the first 100!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'000',\n",
       " u'1000',\n",
       " u'1100',\n",
       " u'12',\n",
       " u'15',\n",
       " u'17',\n",
       " u'1910s',\n",
       " u'1930s',\n",
       " u'1936',\n",
       " u'1944',\n",
       " u'1950s',\n",
       " u'2nd',\n",
       " u'30',\n",
       " u'40',\n",
       " u'abandoned',\n",
       " u'ability',\n",
       " u'abject',\n",
       " u'able',\n",
       " u'about',\n",
       " u'absolute',\n",
       " u'absolution',\n",
       " u'accepts',\n",
       " u'access',\n",
       " u'accidentally',\n",
       " u'accomplish',\n",
       " u'accused',\n",
       " u'action',\n",
       " u'actions',\n",
       " u'acts',\n",
       " u'addition',\n",
       " u'advance',\n",
       " u'advanced',\n",
       " u'adventure',\n",
       " u'adventurous',\n",
       " u'affected',\n",
       " u'african',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'agency',\n",
       " u'agent',\n",
       " u'aging',\n",
       " u'aid',\n",
       " u'alfred',\n",
       " u'alimony',\n",
       " u'alive',\n",
       " u'all',\n",
       " u'alleviate',\n",
       " u'alliance',\n",
       " u'allied',\n",
       " u'allowing',\n",
       " u'allows',\n",
       " u'ally',\n",
       " u'almost',\n",
       " u'alone',\n",
       " u'along',\n",
       " u'also',\n",
       " u'always',\n",
       " u'america',\n",
       " u'american',\n",
       " u'amid',\n",
       " u'amidst',\n",
       " u'amount',\n",
       " u'amusement',\n",
       " u'an',\n",
       " u'ancient',\n",
       " u'and',\n",
       " u'angel',\n",
       " u'angier',\n",
       " u'another',\n",
       " u'answers',\n",
       " u'apartment',\n",
       " u'approach',\n",
       " u'aragorn',\n",
       " u'archeology',\n",
       " u'are',\n",
       " u'area',\n",
       " u'aristocratic',\n",
       " u'ark',\n",
       " u'army',\n",
       " u'around',\n",
       " u'arrives',\n",
       " u'art',\n",
       " u'artifact',\n",
       " u'as',\n",
       " u'ashore',\n",
       " u'ask',\n",
       " u'asks',\n",
       " u'aspires',\n",
       " u'assassin',\n",
       " u'assigned',\n",
       " u'assignment',\n",
       " u'assistants',\n",
       " u'at',\n",
       " u'attack',\n",
       " u'attempt',\n",
       " u'attempts',\n",
       " u'aurelius',\n",
       " u'auschwitz',\n",
       " u'automatically']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.83321334,  4.23867845,  4.23867845, ...,  4.23867845,\n",
       "        2.53393036,  4.23867845])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what is represented, let's go forward with the tfidf array.  Remember that array stores the document (plot summary) tfidf scores (in the order they appear in the dictionary they are stored.\n",
    "\n",
    "Thus, if the first item in plots_50 is \"It's a Wonderful Life\" then `tfidf_array[0]` is the tfidf scores for that over all the words in the corpus.  \n",
    "\n",
    "Let's prove this so ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"George Bailey has spent his entire life giving of himself to the people of Bedford Falls. He has always longed to travel but never had the opportunity in order to prevent rich skinflint Mr. Potter from taking over the entire town. All that prevents him from doing so is George's modest building and loan company, which was founded by his generous father. But on Christmas Eve, George's Uncle Billy loses the business's $8,000 while intending to deposit it in the bank. Potter finds the misplaced money and hides it from Billy. When the bank examiner discovers the shortage later that night, George realizes that he will be held responsible and sent to jail and the company will collapse, finally allowing Potter to take over the town. Thinking of his wife, their young children, and others he loves will be better off with him dead, he contemplates suicide. But the prayers of his loved ones result in a gentle angel named Clarence coming to earth to help George, with the promise of earning his wings. He shows George what things would have been like if he had never been born. In a nightmarish vision in which the Potter-controlled town is sunk in sex and sin, those George loves are either dead, ruined, or miserable. He realizes that he has touched many people in a positive way and that his life has truly been a wonderful one.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plots_50[plots_50.keys()[0]]  # it's a wonderful life is the first movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follow code will just return the scores from the first movie **only** for the terms with a score **greater than 0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.13547779388675404, u'40'),\n",
       " (0.091484972436923995, u'and'),\n",
       " (0.14980820375807655, u'answers'),\n",
       " (0.14980820375807655, u'attack'),\n",
       " (0.27095558777350809, u'bandits'),\n",
       " (0.11742361261454501, u'battle'),\n",
       " (0.14980820375807655, u'culminates'),\n",
       " (0.13547779388675404, u'day'),\n",
       " (0.14980820375807655, u'defend'),\n",
       " (0.14980820375807655, u'fallen'),\n",
       " (0.11742361261454501, u'film'),\n",
       " (0.072151389045991207, u'for'),\n",
       " (0.072151389045991207, u'from'),\n",
       " (0.14980820375807655, u'gathers'),\n",
       " (0.14980820375807655, u'giant'),\n",
       " (0.13547779388675404, u'hard'),\n",
       " (0.078595205258502346, u'has'),\n",
       " (0.072151389045991207, u'he'),\n",
       " (0.10553162742351276, u'help'),\n",
       " (0.089557054150858795, u'him'),\n",
       " (0.13547779388675404, u'how'),\n",
       " (0.049673549354238587, u'in'),\n",
       " (0.14980820375807655, u'meals'),\n",
       " (0.068427617645104688, u'on'),\n",
       " (0.10081220878863621, u'other'),\n",
       " (0.14980820375807655, u'protection'),\n",
       " (0.12531020627335637, u'request'),\n",
       " (0.44942461127422967, u'samurai'),\n",
       " (0.11742361261454501, u'small'),\n",
       " (0.14980820375807655, u'supply'),\n",
       " (0.14980820375807655, u'teach'),\n",
       " (0.16882089033099962, u'the'),\n",
       " (0.13547779388675404, u'themselves'),\n",
       " (0.19329877306142274, u'they'),\n",
       " (0.12531020627335637, u'three'),\n",
       " (0.14980820375807655, u'times'),\n",
       " (0.089648861958749937, u'to'),\n",
       " (0.14980820375807655, u'townspeople'),\n",
       " (0.14980820375807655, u'veteran'),\n",
       " (0.29961640751615309, u'village'),\n",
       " (0.076314211303916052, u'when'),\n",
       " (0.074171546838958768, u'who'),\n",
       " (0.065059056666138634, u'with')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(score, term)  for score, term in zip(tfidf_array[1], vectorizer.get_feature_names()) if score > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the similarity between two documents.\n",
    "\n",
    "We have talked about several distance metrics in previous lectures, and now we can think more about the metrics and less about the implementation.\n",
    "\n",
    "You will find Scikit-learn has a number of [really popular metrics already implemented](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; WARMUP Take the first 10 plot summaries and find the summaries they are most similar to Euclidean and Cosine Similarity\n",
    "\n",
    "* Euclidean distance metrics can be done using [sklearn.metrics.pairwise.euclidean_distances](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html#sklearn.metrics.pairwise.euclidean_distances) and\n",
    "* cosine similarity will be done using [sklearn.metrics.pairwise.cosine_similarity](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity).\n",
    "\n",
    "### &#167; WARMUP Discuss the differences or similarity between the results executing the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "def test_similarity(type):\n",
    "    if type == 'euc':\n",
    "        max_n = 5\n",
    "        sim = euclidean_distances\n",
    "    elif type == 'cos':\n",
    "        max_n = -5\n",
    "        sim = cosine_similarity\n",
    "        \n",
    "    for i in xrange(0,20):\n",
    "        test_data = sim(tfidf_matrix)[i]\n",
    "        #max_n, test_data =  5, euclidean_distances(tfidf_matrix)[i]\n",
    "\n",
    "        if max_n < 0:\n",
    "            top_most_similar = zip(test_data.argsort()[max_n:],test_data[test_data.argsort()[max_n:]])\n",
    "            idx, value = top_most_similar[-2]\n",
    "        else:\n",
    "            top_most_similar = zip(test_data.argsort()[:max_n],test_data[test_data.argsort()[:max_n]])\n",
    "            idx, value = top_most_similar[1]\n",
    "\n",
    "        # print top_most_similar\n",
    "        print \"{} ({}, {}) and {}({}) are most similar.\".format(plots_50.keys()[idx], idx, value, plots_50.keys()[i], i)\n",
    "        \n",
    "test_similarity('cos')   \n",
    "print \"\\n\"\n",
    "test_similarity('euc') \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 / Document Clustering\n",
    "\n",
    "### &#167; Load the 1000 OMDB summaries in to a Python list called `sample_docs`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; Use `TfidfVectorizer` to load the documents into an array\n",
    "\n",
    "You will need to explore how to do this with the [example here]().\n",
    "\n",
    "Once you have your vectorizer and array, you can use the `get_feature_names()` method on `TfidfVectorizer` to get the document terms and their Tfidf scores.  \n",
    "\n",
    "### &#167; Use `get_feature_names()` to display the top 10 terms and their Tfidf scores  of just the first document\n",
    "\n",
    "**HINT**\n",
    "The method and its usage can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167;  Use the KMeans algorithm with $k=20$ to compute 20 clusters over the 1000 summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### &#167; Explore the clusters by printing the sizes (number of items in each cluster) for all of them.  Your answer should be a list with the cluster number and the number of items in that cluster.\n",
    "\n",
    "**HINT:**  You will need to use the attribute `labels_` to get the cluster names (there numeric).  This attribute stores each of the documents (in the order they are store in the matrix) and the cluster \"label\" (number) they belong to.\n",
    "\n",
    "For example after running the KMeans algorithm (fitting the data to the algorithm) and storing the results in a variable `km` then `km.labels_` will return the list (array) of all the items and their cluster number.\n",
    "\n",
    "It would look something like:\n",
    "\n",
    "    array([11,  2,  7, 12,  8,  4, 14, 14,  8, 12,  0,  4, 12,  2, 11,  8, 14,\n",
    "       18, 14,  9,  7,  5,  3,  4, 12, 18,  8, 16, 18,  4, 18,  0,  5,  0,\n",
    "    ...])\n",
    "\n",
    "where each index on the array is the item from your original data and the value for that index the cluster label (as a number).\n",
    "\n",
    "**HINT:** Count the frequency of the cluster labels to answer this question and you can use the [`Counter` object from the Python `collections` library](https://docs.python.org/2/library/collections.html#collections.Counter).\n",
    "\n",
    "**OUTPUT:**\n",
    "Your output will look something this where the key in the dictionary is the cluster label (number) and the value for that key is the count:\n",
    "\n",
    "    Counter({0: 21,\n",
    "             1: 9,\n",
    "             2: 14,\n",
    "             3: 220,\n",
    "             4: 31,\n",
    "             5: 71,\n",
    "             6: 75,\n",
    "             7: 17,\n",
    "             8: 120,\n",
    "             9: 89,\n",
    "             10: 118,\n",
    "             11: 12,\n",
    "             12: 22,\n",
    "             13: 55,\n",
    "             14: 69,\n",
    "             15: 41,\n",
    "             16: 180,\n",
    "             17: 14,\n",
    "             18: 82,\n",
    "             19: 55})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167;  Use Hierarchical clustering algorithm with cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "A common metric used in document / text mining is [cosine similarity](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity), defined as:\n",
    "\n",
    "$$ \\textrm{cos}(\\mathbf{x}, \\mathbf{y}) = { \n",
    "{ \\sum_{i=1}^n {x_i y_i} } \\over \n",
    "{ \\sqrt {\\sum_{i=1}^n x_i^2}} { \\sqrt {\\sum_{i=1}^n y_i^2 }}\n",
    "}$$\n",
    "\n",
    "Where $\\mathbf{x}$ and $\\mathbf{y}$ are two vectors of equal length.  If $\\textrm{cos}(\\mathbf{x}, \\mathbf{y})$ is 0 then $\\mathbf{x}$ and $\\mathbf{y}$ are orthogonal or not correlated.  If 1, then two vectors are the same and if -1, completely opposite.\n",
    "\n",
    "#### Hierarchical Clustering\n",
    "\n",
    "**HINT**\n",
    "\n",
    "To do hierarchical clustering with cosine similarity, read the documentation for [`sklearn.cluster.AgglomorativeClustering`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) and pay close attention to the `affinity` attribute of the input parameters of the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; Compare your $k$-means results with those of the hierarchical clustering - point to at least TWO major differences (e.g. cluster distributions, sizes, etc.)\n",
    "\n",
    "**HINT** \n",
    "\n",
    "Use Python's [`Counter`](https://docs.python.org/dev/library/collections.html) class from `collections` on the `.labels_` attribute of your completed clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; There was a discussion some time ago about adding dendrogram plotting to Scikit-learn (which is still currently not implemented).  Please read that discussion and use the `plot_dendrogram` method below (taken from that discussion) to plot the dendrogram for your hierarchical clusters.\n",
    "\n",
    "**HINT**\n",
    "CAREFULLY READ THE DISCUSSION LINK and ORIGINAL CODE: https://github.com/scikit-learn/scikit-learn/pull/3464\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; Associate the clusters with their top terms and print a list of the cluster number and the genres of that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 / Classification With $k$-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW1 you learned about and used the $k$-NN algorithm.  You computed $k$ neighbors based on actual data.  This algorithm can also be used to do what is called a _lazy learner_ because it learns from the _testing_ phase instead of the _training_ phase.  This has performance issues unto itself since all the data it has to be seen. It can, nonetheless, be used as a way to do **supervised classification** since it has learned all the class labels already.\n",
    "\n",
    "You will first start with just a warmup of the using the [NearestNeigbors algorithm](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) already implented in Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; Using NearestNeighbors in SKL (Scikit-learn) find the 10 nearest neighbors using the movie data you have already begun exploring, storing the data in a variable called neigh\n",
    "\n",
    "For example, you code will look like:\n",
    "\n",
    "``` python\n",
    "...\n",
    "neigh = #<your nearest neighbor class>\n",
    "...\n",
    "```\n",
    "\n",
    "**HINT:** See the examples on how to do this [here](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors.kneighbors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; The function below takes a movie index (from the original data file, in order) and returns the titles of the $k$-nearest neighbors.  It requires you to have set the `neigh` variable to the NearestNeighbor class and have fitted the data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_nn_movie_names(movie_index):\n",
    "    tfidf_array[movie_index]\n",
    "    print(\"{}\\n===\".format(docs[movie_index]['Title']))\n",
    "    for idx, dist in zip(np.nditer(neigh.kneighbors(tfidf_array[movie_index])[1]), \n",
    "                         np.nditer(neigh.kneighbors(tfidf_array[movie_index])[0])):\n",
    "        if idx != movie_index: \n",
    "            print(u'{}'.format(docs[idx]['Title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; Use `get_nn_movie_names(index)` for the indices and associated movies below.  In one or two sentences give your opinion of how well this method compares to the lists in the links (this is not meant to be scientific). \n",
    "| index | movie name | similar movies link |\n",
    "|-------|------------|---------------------|\n",
    "| 34 | Saving Private Ryan | http://www.similarkind.com/search?q=saving+private+ryan |\n",
    "| 26 | Life is Beautiful | http://www.similarkind.com/search?q=the+green+mile |\n",
    "| 199 | Hotel Rwanda | http://www.similarkind.com/search?q=hotel+rwanda |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; In 1 or 2 sentences suggest ways to improve the results of the algorithm (e.g. more data, better features, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 / Classification With Decision Trees\n",
    "\n",
    "\n",
    "As we learned, decision trees are a powerful way to build classifiers, especially since the output is interpretable.  By using information gain such as entropy and gini coefficient, nodes can be chosen that split the data in meaningful ways allowing the leaf nodes to provide the labels of a set of decisions as one follows each attribute at a decision point.\n",
    "\n",
    "Scikit-learn [implements decision tree classifiers](http://scikit-learn.org/stable/modules/tree.html#tree) and we will use them for this part of the assignment.  To use this classifier, you will need to remember the following import\n",
    "\n",
    "``` python\n",
    "from sklearn import tree\n",
    "```\n",
    "\n",
    "As always, you should include these imports as well\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA FOR PART 3\n",
    "\n",
    "We will be using data from the US Federal [Consumer Financial Protection Bureau (CFPB)](http://www.consumerfinance.gov/data-research/consumer-complaints/) which maintains a consumer complaint database.  In it there are a number of factors being stored, including company names, consumer narratives, complaint type, date, etc.\n",
    "\n",
    "The dataset can be downloaded in its entirety [here](http://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data) as a CSV or JSON file and you will also find it in the [`data`](./data/) directory for this repo.  That file was retrieved on October 30, 2016 and represents data from March 2015 to August 2016 in about 27K rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; Read the complaint data into a Pandas Dataframe called `df` and restrict the data to 4 columns : `Product`, `Sub-product`, `Issue` and `Sub-issue`.\n",
    "\n",
    "### &#167; Create a training dataset with 3000 random data points from `df` and call that `df_train`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINTS\n",
    "\n",
    "##### RESTRICTING DATAFRAME COLUMNS\n",
    "Recall an easy way to restrict the columns of a dataframe :\n",
    "\n",
    "```python\n",
    "df[['col1', 'col2', 'coln']]\n",
    "```\n",
    "\n",
    "returns the dataframe with just `col1`, `col2`, ...\n",
    "\n",
    "##### RANDOM NUMBERS IN NUMPY\n",
    "Explore [`numpy.random.choice`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) which should help you understand how to obtain 3000 random numbers within the row  index range of your data.  Pay attention to the `replace` parameter so you do not get duplicates.\n",
    "\n",
    "You will store this because it will be needed again later -- maybe use the variable `train_idxs`.\n",
    "\n",
    "##### USING `DATAFRAME.IX[list_of_indices]` TO GET A SET OF ROWS BY INDEX\n",
    "\n",
    "Review the [Pandas docs on selecting data](http://pandas.pydata.org/pandas-docs/stable/indexing.html) to learn about `ix`.\n",
    "\n",
    "Consider what this example does,\n",
    "\n",
    "```python\n",
    "# assume df is a Dataframe with data in it\n",
    "\n",
    "train_idxs = [0, 9, 14]\n",
    "df.ix[train_idxs] \n",
    "# returns the dataframe with row 1, 10, 15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#167; Notice that the data are all categorical.  Most algorithms required a little work to get the data into a form that can be easily used.  Binarize `df_train` so that is it ready to be used in most typical scenarios.\n",
    "\n",
    "#### HINTS\n",
    "\n",
    "**label_binarize**\n",
    "One way to do this is to explore  [label_binarize](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label_binarize.html).  You will need to write a lot more code (fewer than 10 lines) but you can do it.\n",
    "\n",
    "**DictVectorizer**\n",
    "Another way is to use [`DictVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) which is part of the feature extraction library of Scikit-learn.  \n",
    "\n",
    "You should see how this trick is done from http://fastml.com/converting-categorical-data-into-numbers-with-pandas-and-scikit-learn/ as inspiration for your solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is ready! We are going to use the data to build a decision tree to classify the company response.  We might be able to use this data, for example, to understand which future compaints may need more attention by the agency.  If we build a good classifier, we might just be able to reduce the time, cost and staff stress.\n",
    "\n",
    "### &#167; Build a DecisionTree from the training data in df_train.\n",
    "\n",
    "\n",
    "#### HINTS\n",
    "\n",
    "**DecisionTreeClassifier**\n",
    "\n",
    "You will most certainly need to use [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Read up on this classfier and use it with `df_train`.\n",
    "\n",
    "**Labeled data**\n",
    "\n",
    "Your classifier is going to be more interesting if it can distinguish between the different classes of company response.\n",
    "\n",
    "To get all the labels (for all your df rows WITH THE SAME INDEX AS YOUR TRAINING DATA):\n",
    "\n",
    "```python\n",
    "df[['Company response to consumer']].ix[[]]\n",
    "```\n",
    "\n",
    "should do the trick.  But this will get them for all rows.\n",
    "\n",
    "Explore [Pandas.unique](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) to understand just how to get the set of labels you need.\n",
    "\n",
    "### &#167; Draw the decision tree.\n",
    "\n",
    "#### HINTS\n",
    "\n",
    "**You can use the code from the demo in scikit-learn or build your own**\n",
    "\n",
    "Use these to help you understand the code snippet below:\n",
    "* `dtc` is your decision tree from the above\n",
    "* `DV.feature_names_` is the DictVectorizer from above\n",
    "* `target_names` are the class names\n",
    "\n",
    "``` python\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "\n",
    "dot_data = tree.export_graphviz(dtc, out_file=\"tree.dot\", \n",
    "                         feature_names=DV.feature_names_,  \n",
    "                         class_names=target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True) \n",
    "\n",
    "graph = pydotplus.graph_from_dot_file(\"tree.dot\")  \n",
    "graph.write_png(\"tree.png\")\n",
    "```\n",
    "\n",
    "### &#167; Look at the decision tree (stored in your PNG file). List 2 reasons why this WOULD or WOULD NOT be a good classifier for the company response. Turn in your PNG file along with your IPYNB file.\n",
    "\n",
    "### &#167; Compute the Precision and Recall for this classifier - remember you will need to test on data you have not already seen!\n",
    "\n",
    "#### HINTS\n",
    "Instead of only pulling 3000 data points you might pull 6000 ad split -- using 3000 for test and 3000 for training.  There are, of course, other ways to do it as discussed in class.\n",
    "\n",
    "* You could consider Scikit-learn's [`model_selection.train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)\n",
    "\n",
    "* You will need to understand how [DecisionTreeClassifier.predict()](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict)\n",
    "\n",
    "* You might want to explore the [metrics capabilities of Scikit-learn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) -- especially the `confusion_matrix()`, `precision_score()` and `recall_score()` methods.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
